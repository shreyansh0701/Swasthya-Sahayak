{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzzXtodf1hoQ",
        "outputId": "0efd56c5-9ced-45bb-b768-9d028bd47b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "gHKPjT4ZRK-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Google_Hackathon/FINAL_dataset2 (1).csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "KPELoW6XRLjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.info()\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "hf0GQmNwRS32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset from CSV file\n",
        "file_path = '/content/drive/MyDrive/Google_Hackathon/FINAL_dataset2 (1).csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the original dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(df.isnull().sum())  # Display the count of missing values in each column\n",
        "\n",
        "df['Symptom_1'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_2'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_3'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_4'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_5'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_6'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_7'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_8'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_9'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_10'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_11'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_12'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_13'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_14'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_15'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_16'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "df['Symptom_17'].fillna('NO OTHER SYMPTOM', inplace=True)\n",
        "\n",
        "# Drop rows with any missing values\n",
        "df['Specialization'] = df.dropna()\n",
        "df['Name'] = df.dropna()\n",
        "df['State'] = df.dropna()\n",
        "df['City'] = df.dropna()\n",
        "df_cleaned = df.dropna()\n",
        "# Display the cleaned dataset\n",
        "print(\"\\nDataset after dropping rows with missing values:\")\n",
        "print(df_cleaned)\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "cleaned_file_path = '/content/drive/MyDrive/Google_Hackathon/FINAL_dataset2 (1).csv'\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"\\nCleaned dataset saved to: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "id": "KIEIpVkQRX9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Google_Hackathon/FINAL_dataset2 (1).csv')\n",
        "\n",
        "# Identify and count duplicate rows\n",
        "duplicate_mask = df.duplicated()\n",
        "num_duplicates = duplicate_mask.sum()\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "\n",
        "# Remove duplicate rows\n",
        "df_cleaned = df.drop_duplicates()\n",
        "\n",
        "# Save the cleaned DataFrame back to a CSV file\n",
        "df_cleaned.to_csv('/content/drive/MyDrive/Google_Hackathon/FINAL_dataset2 (1).csv', index=False)\n",
        "\n",
        "print(\"Duplicate rows removed and cleaned dataset saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JufoU-7jVmLz",
        "outputId": "5021a15a-1223-4779-d69c-9eccecac30e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 115250\n",
            "Duplicate rows removed and cleaned dataset saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Google_Hackathon/FINAL_dataset2 (1).csv')\n",
        "\n",
        "# Identify object columns for label encoding\n",
        "object_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode object columns and store the mapping\n",
        "for col in object_columns:\n",
        "    # Fit and transform the column with LabelEncoder\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "    # Store the mapping of original values to encoded values\n",
        "    mapping_dict = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "    # Add the mapping to a new column in the DataFrame\n",
        "    df[f'{col}_Encoded'] = df[col]\n",
        "\n",
        "# Save the modified DataFrame back to a new CSV file\n",
        "encoded_csv_path = '/content/drive/MyDrive/Google_Hackathon/encoded_dataset.csv'\n",
        "df.to_csv(encoded_csv_path, index=False)\n",
        "\n",
        "print(f\"Encoded dataset saved to {encoded_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8_tNTnnXR1f",
        "outputId": "04d763a2-385b-48f4-d930-4b0cd5cd6564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded dataset saved to /content/drive/MyDrive/Google_Hackathon/encoded_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define the file paths\n",
        "input_file_path = '/content/drive/MyDrive/Google_Hackathon/encoded_dataset.csv'\n",
        "output_file_path = '/content/drive/MyDrive/Google_Hackathon/encoded_dataset.csv'\n",
        "\n",
        "# Load the dataset from CSV file\n",
        "df = pd.read_csv(input_file_path)\n",
        "\n",
        "# Identify numerical columns for scaling\n",
        "numerical_columns = df.select_dtypes(include=['int', 'float']).columns\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling to numerical columns\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Save the processed dataset with normalized values to CSV\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Processed dataset saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sjunRdLZRvB",
        "outputId": "79f4be95-6233-45ae-87d2-6914b51a6c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed dataset saved to /content/drive/MyDrive/Google_Hackathon/encoded_dataset.csv\n"
          ]
        }
      ]
    }
  ]
}